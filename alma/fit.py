import os
import numpy as np
import emcee
import scipy.optimize
import matplotlib.pyplot as plt
import corner
import galario.double as gd
from galario import arcsec

from . import image

'''Wrappers to help with fitting and automation.

Bascially a copy of the example jupyter notebook script. A call
could look something like this.

```
f = alma.fit.Fit(uv_file='uv.txt')
f.init_image(p0=[0,0,30,50,0.014,2.2,0.3,0.3])
f.optimise()
f.mcmc(nwalk=16, nthreads=4, nsteps=10, burn=5)
```

'''

class Fit(object):
    '''Class for fitting and automation.'''

    def __init__(self, uv_file=None):
        '''Get an object for fitting.
            
        Parameters
        ----------
        uv_file : str
            File with visiblities.
        '''

        if uv_file is not None:
            self.load_vis(uv_file)


    def load_vis(self, uv_file, reweight=True,
                 img_sz_kwargs={}):
        '''Read in a file with visibilities.
            
        Parameters
        ----------
        uv_file : str
            Name of file, generated by uvplot, to load.
        reweight : bool, optional
            Reweight so that null model has chi squared = 1.
        img_s_kwargs : dict
            Keywords to pass to galario.get_image_size.
        '''
        
        # get the file, assume we're getting the output from uvplot
        u, v, Re, Im, w = np.require(np.loadtxt(uv_file,unpack=True),
                                     requirements=["C_CONTIGUOUS"])

        # meaning we can get the mean wavelength like so
        with open(uv_file) as f:
            _ = f.readline()
            tmp = f.readline()

        wavelength = float(tmp.strip().split('=')[1])
        print('wavelength is {} mm'.format(wavelength*1e3))
            
        self.wavelength = wavelength
        self.u = u / wavelength
        self.v = v / wavelength
        self.re = u
        self.im = u

        # re-weight so that chi^2 for null model is 1
        reweight_factor = np.sum( ( Re**2.0 + Im**2.0) * w ) / len(w)
        if reweight:
            print('reweighting factor used {}'.format(reweight_factor))
            w /= reweight_factor
        else:
            print('reweighting factor unused {}'.format(reweight_factor))

        self.w = w

        # get image pixel scale and size
        self.nxy, self.dxy = gd.get_image_size(self.u, self.v,
                                               **img_sz_kwargs,
                                               verbose=True)
        self.dxy_arcsec = self.dxy / arcsec


    def init_image(self, p0=None,
                   image_kwargs={'dens_model':'gauss_3d',},
                   compute_rmax_kwargs={}):
        '''Initialise Image object.
        
        Parameters
        ----------
        p0 : list
            List of initial parameters.
        **image_kwargs : dict
            Args to pass on to image.Image.
        **compute_rmax_kwargs : dict
            Args to pass on to image.Image.compute_rmax.
        '''
        self.img = image.Image(arcsec_pix=self.dxy_arcsec,
                              image_size=(self.nxy, self.nxy),
                              wavelength=self.wavelength,
                              **image_kwargs)


        self.p0 = p0

        # get rmax for these parameters
        self.img.compute_rmax(p0, **compute_rmax_kwargs)


    def lnprob(self, p):
        '''Log of posterior probability function.'''

        for i in range(len(p)):
            if p[i] < self.img.p_ranges[i][0] or p[i] > self.img.p_ranges[i][1]:
                return -np.inf

        # we generate the image with PA = North, including primary beam correction
        image = self.img.image(p[3:]) * self.img.pb
        
        # galario  translates and rotates it for us
        image = np.flipud(image)  # galario expects origin=upper
        chi2 = gd.chi2Image(image, self.dxy,
                            self.u, self.v, self.re, self.im, self.w,
                            dRA = p[0]*arcsec, dDec = p[1]*arcsec,
                            PA = np.deg2rad(p[2]) )
        return -0.5 * chi2


    def nlnprob(self, p):
        '''Negative log probability, for minimising.'''
        return -self.lnprob(p)


    def optimise(self, niter=100):
        '''Optimise parameters.
        
        Parameters
        ----------
        niter : int
            Number of optimisation iterations.
        '''
        res = scipy.optimize.minimize(self.nlnprob, self.p0,
                                      method='Nelder-Mead',
                                      options={'maxiter':niter})
        print('Best parameters: {}'.format(res['x']))
        self.p0 = np.array(res['x'])


    def mcmc(self, nwalk=16, nsteps=10, nthreads=1, burn=5,
             out_dir=''):
        '''Do the mcmc.
        
        Parameters
        ----------
        nwalk : int
            Number of walkers.
        nsteps : int
            Number of mcmc steps.
        nthreads : int
            Number of threads.
        burn : int
            Number of steps to count as burn in.
        out_dir : str
            Where to put results.
        '''

        sampler = emcee.EnsembleSampler(nwalk, self.img.n_params,
                                        self.lnprob, threads=nthreads)

        # initialize the walkers
        pos = [self.p0 + self.p0*0.1*np.random.randn(self.img.n_params)
                                           for i in range(nwalk)]

        # execute the MCMC
        pos, prob, state = sampler.run_mcmc(pos, nsteps)

        model_name = self.img.Dens.model
        if not os.path.exists(out_dir):
            os.mkdir(out_dir)

        # see what the chains look like, skip a burn in period if desired
        fig,ax = plt.subplots(self.img.n_params+1,2,
                              figsize=(9.5,5),sharex='col',sharey=False)

        for j in range(nwalk):
            ax[-1,0].plot(sampler.lnprobability[j,:burn])
            for i in range(self.img.n_params):
                ax[i,0].plot(sampler.chain[j,:burn,i])
                ax[i,0].set_ylabel(self.img.params[i])

        for j in range(nwalk):
            ax[-1,1].plot(sampler.lnprobability[j,burn:])
            for i in range(self.img.n_params):
                ax[i,1].plot(sampler.chain[j,burn:,i])
                ax[i,1].set_ylabel(self.img.params[i])

        ax[-1,0].set_xlabel('burn in')
        ax[-1,1].set_xlabel('sampling')
        fig.savefig(out_dir+'/chains-'+model_name+'.png')

        # make the corner plot
        fig = corner.corner(sampler.chain[:,burn:,:].reshape((-1,self.img.n_params)),
                            labels=self.img.params,show_titles=True)

        fig.savefig(out_dir+'/corner-'+model_name+'.png')

        # get the median parameters
        self.p = np.median(sampler.chain[:,burn:,:].reshape((-1,self.img.n_params)),
                           axis=0)
        self.s = np.std(sampler.chain[:,burn:,:].reshape((-1,self.img.n_params)),
                        axis=0)
        print('best fit parameters: {}'.format(self.p))

        # recompute the limits for the full rotated image
        self.img.compute_rmax(self.p, image_full=True)

        fig,ax = plt.subplots()
        ax.imshow(self.img.image_full(self.p)[self.img.cc], origin='bottom')
        fig.savefig(out_dir+'/best-'+model_name+'.png')

        # save the chains to file
        np.savez_compressed(out_dir+'/chains-'+model_name+'.npz',
                            sampler.chain, sampler.lnprobability)

        # save the visibilities for subtraction from the data
        vis_mod = gd.sampleImage(self.img.pb * self.img.image(self.p[3:]),
                                 self.dxy, self.u, self.v,
                                 dRA = self.p[0]*arcsec, dDec = self.p[1]*arcsec,
                                 PA = np.deg2rad(self.p[2]) )
        np.save(out_dir+'/vis-'+model_name+'.npy', vis_mod)

